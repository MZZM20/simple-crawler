这是我的第一个爬虫项目.
今天早上起来,突然想把网页上的小说下载到本地来看.
自然而然地,我想到了爬虫.

上网参考了一下别人的代码,自己也做了一些尝试.
大概在12:00左右完成了第一版代码,之后又不断调试.

最终,在2:30左右才最终完成.

若是想使用这个项目,我希望你得有以下知识点:

1.python 正则表达式
2.python requests包的简单基础

当然,如果你不是小白,就不必再往下看了,我写的是很粗糙的代码.


具体思想:

首先,我通过python requests模块获得网页源代码.
就是你使用chrme浏览器时,右键然后点击的"查看网页源代码",此时你会发现在一串串代源中会夹杂着汉字,而这些汉字就是我们想要的东西.

接着,我们使用python re模块的正则表达式来将网页源码中的汉字提取出来.
这里要注意了,就我目前所发现的,网页源码一般有两种编码格式:"utf-8"和"gbk2312".如果使用错误的编码格式解析网页源码的话,你会得到一堆乱码.

然后,就是换页的问题.
一开始你会有预设好的网址,但是将这一页爬下来了后,接着该怎么办呢?
再手动点击"下一页",得到网址,然后再改动代码```这样显然是不智的.
我们可以从网页中提取出下一页的网址.
我以"神凑轻小说文库"网站的书籍阅读"约会大作战"为例,查看它的网页源代码.
你会发现<A href="4791.html"><img src="http://www.shencou.com/logo/x.gif" alt="下一章" /></A><span style="color: red;"></span> 这样一段源码(可以CTRL+f搜索,将这段源码丢进去搜索框中,然后你也可以看到),其中"4791.html"再加上"www.shencou.com/read/0/106/"就是下一页的网址了.

最后是终止问题.即:书籍爬取完成后,或者是爬取过程中出现了异常,我们如何让程序正常退出呢?
这里需要用到一点点Python 异常处理的知识.
经观察得知,一般书籍到了最后一页,它要么是不会再有下一页的网址了,
这时代码:Next = re.search(r'<A href="(?P<content>.*)"><img src="http://www.shencou.com/logo/x.gif" alt="下一章"',response.text,re.I).groupdict()['content']   #抓取下一页信息
会抛出异常,使用except接受,处理使程序退出就好了;要么是再后面的网页上不会有像原来一样的格式了(即'章节标题'+'章节内容'),此时也抛出异常,处理使程序退出就好了.

至此,整个程序结束.
我尝试了一下,一份6804kb的书籍,只需要三分钟就可以完全下载到本地了.
好了,赶快拿去尝试一下吧!
